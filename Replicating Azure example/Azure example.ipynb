{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashv\\.jupyter\\Python notebooks\\support-tickets-classification\n",
      "C:\\Users\\mashv\\.jupyter\\Python notebooks\\support-tickets-classification\\Input_dataset\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "data_folder = \"\\Input_dataset\\\\\"\n",
    "get_t = os.getcwd()\n",
    "print(get_t +  data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Endava support tickets dataset...\n",
      "File: C:\\Users\\mashv\\.jupyter\\Python notebooks\\support-tickets-classification\\Input_dataset\\all_tickets.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import os\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "def download_file(file_url, folder_path, data_folder, file_name):\n",
    "#     file_path = os.path.join(folder_path, data_folder, file_name)\n",
    "    file_path = os.getcwd()+ data_folder+ file_name\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file from ' + file_url + '...')\n",
    "        urlretrieve(file_url, file_path)\n",
    "        print('Done downloading file: '+file_path)\n",
    "        input_file= pd.read_csv(url)\n",
    "    else:\n",
    "        print('File: ' + file_path + ' already exists.')\n",
    "        input_file= pd.read_csv(os.getcwd()+data_folder + file_name)\n",
    "    return input_file\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "    print('Downloading Endava support tickets dataset...')\n",
    "    folder_path =   os.getcwd() #os.path.join(         os.path.dirname(             os.path.abspath(__file__)        ),        'datasets'    )\n",
    "    url = \"https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/datasets/all_tickets.csv\"\n",
    "    \n",
    "    file_name = 'all_tickets.csv'\n",
    "    input_data = download_file(url, folder_path, data_folder, file_name)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_data= download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>ticket_type</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category1</th>\n",
       "      <th>sub_category2</th>\n",
       "      <th>business_service</th>\n",
       "      <th>urgency</th>\n",
       "      <th>impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hi since recruiter lead permission approve req...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>connection with icon</td>\n",
       "      <td>icon dear please setup icon per icon engineers...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work experience user</td>\n",
       "      <td>work experience user hi work experience studen...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>requesting for meeting</td>\n",
       "      <td>requesting meeting hi please help follow equip...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reset passwords for external accounts</td>\n",
       "      <td>re expire days hi ask help update passwords co...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0                                    NaN   \n",
       "1                   connection with icon   \n",
       "2                   work experience user   \n",
       "3                 requesting for meeting   \n",
       "4  reset passwords for external accounts   \n",
       "\n",
       "                                                body  ticket_type  category  \\\n",
       "0  hi since recruiter lead permission approve req...            1         4   \n",
       "1  icon dear please setup icon per icon engineers...            1         6   \n",
       "2  work experience user hi work experience studen...            1         5   \n",
       "3  requesting meeting hi please help follow equip...            1         5   \n",
       "4  re expire days hi ask help update passwords co...            1         4   \n",
       "\n",
       "   sub_category1  sub_category2  business_service  urgency  impact  \n",
       "0              2             21                71        3       4  \n",
       "1             22              7                26        3       4  \n",
       "2             13              7                32        3       4  \n",
       "3             13              7                32        3       4  \n",
       "4              2             76                 4        3       4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_type\n",
       "0    13928\n",
       "1    34621\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.groupby('ticket_type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (48549, 9)\n",
      "Number of classes before removing classes with less then 1 rows: 2\n",
      "Shape of dataset after removing classes with less then 1 rows: (48549, 9)\n",
      "Number of classes after removing classes with less then 1 rows: 2\n",
      "Training NB classifier\n",
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[2473  249]\n",
      " [ 127 6861]]\n",
      "Mean without GridSearch: 0.9612770339855818\n",
      "Confusion matrix with GridSearch:\n",
      "[[2643   79]\n",
      " [ 171 6817]]\n",
      "Mean with GridSearch: 0.9742533470648815\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      2722\n",
      "           1       0.99      0.98      0.98      6988\n",
      "\n",
      "    accuracy                           0.97      9710\n",
      "   macro avg       0.96      0.97      0.97      9710\n",
      "weighted avg       0.97      0.97      0.97      9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "\n",
    "column_to_predict = \"ticket_type\"\n",
    "# column_to_predict = \"business_service\" \n",
    "# column_to_predict = \"category\" \n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"NB\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "# classifier = \"SVM\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # TODO Add download dataset\n",
    "     \n",
    "    # loading dataset from dprep in Workbench    \n",
    "    # dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "    # loading dataset from csv\n",
    "    file_name = 'all_tickets.csv'\n",
    "    data_folder = \"\\Input_dataset\\\\\"\n",
    "    \n",
    "    dfTickets = pd.read_csv((os.getcwd() + data_folder + file_name) , dtype=str)  \n",
    "\n",
    "    text_columns = \"body\"  # \"title\" - text columns used for TF-IDF\n",
    "    \n",
    "    # Removing rows related to classes represented by low amount of data\n",
    "    print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "    print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "    bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "    tags = bytag[bytag.body > min_data_per_class].index\n",
    "    dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "    print(\n",
    "        \"Shape of dataset after removing classes with less then \"\n",
    "        + str(min_data_per_class) + \" rows: \"\n",
    "        + str(dfTickets.shape)\n",
    "    )\n",
    "    print(\n",
    "        \"Number of classes after removing classes with less then \"\n",
    "        + str(min_data_per_class) + \" rows: \"\n",
    "        + str(len(np.unique(dfTickets[column_to_predict])))\n",
    "    )\n",
    "\n",
    "    labelData = dfTickets[column_to_predict]\n",
    "    data = dfTickets[text_columns]\n",
    "\n",
    "    # Split dataset into training and testing data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data, labelData, test_size=0.2\n",
    "    )  # split data to train/test sets with 80:20 ratio\n",
    "\n",
    "    # Extracting features from text\n",
    "    # Count vectorizer\n",
    "    if remove_stop_words:\n",
    "        count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "        count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "    elif use_stemming:\n",
    "        count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "    else:\n",
    "        count_vect = CountVectorizer()\n",
    "\n",
    "    # Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "    if classifier == \"NB\":\n",
    "        print(\"Training NB classifier\")\n",
    "        # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "        # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "        # We will be using the 'text_clf' going forward.\n",
    "\n",
    "        text_clf = Pipeline([\n",
    "            ('vect', count_vect),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "        ])\n",
    "        text_clf = text_clf.fit(train_data, train_labels)\n",
    "\n",
    "    elif classifier == \"SVM\":\n",
    "        print(\"Training SVM classifier\")\n",
    "        # Training Support Vector Machines - SVM\n",
    "        text_clf = Pipeline([(\n",
    "            'vect', count_vect),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', SGDClassifier(\n",
    "                loss='hinge', penalty='l2', alpha=1e-3,\n",
    "                max_iter=5, random_state=42\n",
    "            )\n",
    "        )])\n",
    "        text_clf = text_clf.fit(train_data, train_labels)\n",
    "\n",
    "    if use_grid_search:\n",
    "        # Grid Search\n",
    "        # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "        # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "        # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "        \n",
    "        # NB parameters\n",
    "        parameters = {\n",
    "            'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__alpha': (1e-2, 1e-3)\n",
    "        }\n",
    "\n",
    "#         SVM parameters\n",
    "#         parameters = {\n",
    "#            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#            'vect__max_features': (None, 5000, 10000, 50000),\n",
    "#            'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "#            'tfidf__use_idf': (True, False),\n",
    "#            'tfidf__norm': ('l1', 'l2'),\n",
    "#            'clf__alpha': (0.00001, 0.000001),\n",
    "#            'clf__penalty': ('l2', 'elasticnet'),\n",
    "#            'clf__max_iter': (10 , 15) #, 50, 80),\n",
    "#         }\n",
    "\n",
    "        # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "        # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "        gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "        gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "        # To see the best mean score and the params, run the following code\n",
    "        gs_clf.best_score_\n",
    "        gs_clf.best_params_\n",
    "\n",
    "    print(\"Evaluating model\")\n",
    "    # Score and evaluate model on test data using model without hyperparameter tuning\n",
    "    predicted = text_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix without GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "    # Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "    if use_grid_search:\n",
    "        predicted = gs_clf.predict(test_data)\n",
    "        prediction_acc = np.mean(predicted == test_labels)\n",
    "        print(\"Confusion matrix with GridSearch:\")\n",
    "        print(metrics.confusion_matrix(test_labels, predicted))\n",
    "        print(\"Mean with GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "    # Ploting confusion matrix with 'seaborn' module\n",
    "    # Use below line only with Jupyter Notebook\n",
    "    # %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    mat = confusion_matrix(test_labels, predicted)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.set()\n",
    "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=np.unique(test_labels),\n",
    "                yticklabels=np.unique(test_labels))\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label')\n",
    "    # Save confusion matrix to outputs in Workbench\n",
    "    # plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Printing classification report\n",
    "    # Use below line only with Jupyter Notebook\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_labels, predicted,\n",
    "                                target_names=np.unique(test_labels)))\n",
    "\n",
    "    # Save trained models to /output folder\n",
    "    # Use with Workbench\n",
    "    if use_grid_search:\n",
    "        pickle.dump(\n",
    "            gs_clf,\n",
    "            open(os.path.join('.', 'outputs', column_to_predict+\".model\"),'wb')\n",
    "        )\n",
    "    else:\n",
    "        pickle.dump(\n",
    "            text_clf,\n",
    "            open(os.path.join(\n",
    "                '.', 'outputs', column_to_predict+\".model\"),\n",
    "                'wb'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30697    1\n",
       "14393    1\n",
       "4338     1\n",
       "18848    1\n",
       "31344    1\n",
       "Name: ticket_type, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.fit of Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 StemmedCountVectorizer(analyzer='word', binary=False,\n",
       "                                        decode_error='strict',\n",
       "                                        dtype=<class 'numpy.int64'>,\n",
       "                                        encoding='utf-8', input='content',\n",
       "                                        lowercase=True, max_df=1.0,\n",
       "                                        max_features=None, min_df=1,\n",
       "                                        ngram_range=(1, 1), preprocessor=None,\n",
       "                                        stop_words='english',\n",
       "                                        strip_accents=None,\n",
       "                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                        tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
